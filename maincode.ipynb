{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Overview\n",
    "- In this step, we load the marketing data from the AI marketing platform.\n",
    "- The dataset includes the following features: `Clicks`, `Exposure`, `Budget`, `Conversion Volume`, and `Search Index`.\n",
    "- Letâ€™s begin by checking for missing values and understanding the data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Step 1: Load the data\n",
    "# Assume the data is in 'marketing_data.csv'\n",
    "data = pd.read_csv('data/marketing_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Cleaning\n",
    "- We remove any rows with missing values and cap outliers to avoid skewed analysis.\n",
    "- Next, we will standardize the features so they have the same scale for clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values)\n",
    "\n",
    "# Remove rows with missing values (optional based on your case)\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Cap outliers at the 99th percentile for key numerical features\n",
    "for col in ['Clicks', 'Conversion_Volume']:\n",
    "    upper_limit = data_clean[col].quantile(0.99)\n",
    "    data_clean[col] = np.where(data_clean[col] > upper_limit, upper_limit, data_clean[col])\n",
    "\n",
    "# Display summary statistics to check data distribution\n",
    "data_clean.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Standardization\n",
    "- Data standardization ensures that features like `Clicks`, `Exposure`, and `Budget` are on the same scale, which improves the performance of clustering algorithms like KMeans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "# Standardize the numerical features\n",
    "data_clean[['Clicks', 'Conversion_Volume', 'Exposure', 'Budget', 'Search_Index']] = scaler.fit_transform(\n",
    "    data_clean[['Clicks', 'Conversion_Volume', 'Exposure', 'Budget', 'Search_Index']]\n",
    ")\n",
    "\n",
    "# Check the first few rows of the standardized data\n",
    "data_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: KMeans Clustering\n",
    "- We apply KMeans clustering to group the ads into 3 clusters.\n",
    "- The `Silhouette Score` provides a measure of how well the clusters are separated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: KMeans Clustering\n",
    "# We will try clustering the data into 3 clusters (this can be changed)\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "data_clean['Cluster'] = kmeans.fit_predict(data_clean[['Clicks', 'Conversion_Volume', 'Exposure', 'Budget', 'Search_Index']])\n",
    "\n",
    "# Display the cluster centers\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n",
    "\n",
    "# Evaluate the silhouette score for the clustering\n",
    "silhouette_avg = silhouette_score(data_clean[['Clicks', 'Conversion_Volume', 'Exposure', 'Budget', 'Search_Index']], data_clean['Cluster'])\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Scatter Plot Visualization\n",
    "- The scatter plot shows the distribution of `Clicks` vs `Conversion Volume`, colored by the assigned cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize Clustering Results (Scatter Plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Clicks', y='Conversion_Volume', hue='Cluster', data=data_clean, palette='viridis', s=100)\n",
    "plt.title('KMeans Clustering of Marketing Data (Clicks vs. Conversion Volume)')\n",
    "plt.xlabel('Clicks')\n",
    "plt.ylabel('Conversion Volume')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Heatmap of Conversion Volume by Industry\n",
    "- The heatmap shows the average conversion volume by industry and cluster, helping to visualize which clusters perform better across industries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualize Clustering by Industry with Heatmap\n",
    "# Group data by Industry and Cluster to calculate average conversion volume\n",
    "heatmap_data = data_clean.groupby(['Industry', 'Cluster'])['Conversion_Volume'].mean().unstack()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Heatmap of Conversion Volume by Industry and Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Industry')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
